{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Farhan Mahbub\n",
    "# CAP5636 - Advanced AI\n",
    "# November 17, 2024\n",
    "# Homework 4: Petting a warg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Petting a warg\n",
    "\n",
    "Wargs do not make good pets. They are vicious creatures, populating Middle Earth, the world described by novels of John Ronald Reuel Tolkien. They tend to show up in the worst moment possible. They eat humans, hobbits, elves and wizards (when they can get them).\n",
    "\n",
    "![A warg, getting ready for breakfast w:300px](figures/Gundabad_Wargs.jpg)\n",
    "\n",
    "Your relationship with a warg can be in the following states:\n",
    "```\n",
    "SleepingWarg\n",
    "AngryWarg\n",
    "FuriousWarg\n",
    "ApoplecticWarg\n",
    "Safe\n",
    "Sorry \n",
    "```\n",
    "\n",
    "![tes](figures/WargStates.jpg)\n",
    "\n",
    "Your actions are limited to petting a warg or striking it with your sword. The transitions are described in the following picture. The safe and sorry states are terminal, where no further actions can be taken. Landing into them has the reward +10 and -10 respectively. All other actions have a reward of -1. \n",
    "\n",
    "The discount factor is $\\gamma=0.9$\n",
    "\n",
    "![](figures/PetAWarg.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to solve this homework\n",
    "The following problems you can solve either with the help of an LLM or by hand. \n",
    "\n",
    "* If you are solving by hand, make sure that you add sufficient comments to make sure that the code is understandable. \n",
    "* If you are solving using an LLM, add in form of comments\n",
    "    * the LLM used (at the first use instance)\n",
    "    * the prompt used to elicit the code\n",
    "    * modifications that had to be done to the code \n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "# --- LLM used: ChatGPT 4.5\n",
    "# --- LLM prompt\n",
    "# Write a python class to encapsulate the least common multiple algorithm\n",
    "# --- End of LLM prompt\n",
    "```\n",
    "\n",
    "The programming language should be Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1: MDP implementation \n",
    "\n",
    "Write a class to implement an MDP. Do not include value or policy iteration in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: ['SleepingWarg', 'AngryWarg', 'FuriousWarg', 'ApoplecticWarg', 'Safe', 'Sorry']\n",
      "Actions in 'SleepingWarg': ['pet', 'strike']\n",
      "Transitions for petting 'SleepingWarg': [('AngryWarg', 0.95, -1), ('Safe', 0.05, 10)]\n"
     ]
    }
   ],
   "source": [
    "# ChatGPT-4o\n",
    "# Provided the problem context and asked it to generate the code for the MDP class.\n",
    "# Had to tweak the transitions, rewards, discount factor, and probabilities to fit the problem context.\n",
    "\n",
    "class WargMDP:\n",
    "    def __init__(self):\n",
    "        # Define the states\n",
    "        self.states = [\n",
    "            \"SleepingWarg\",\n",
    "            \"AngryWarg\",\n",
    "            \"FuriousWarg\",\n",
    "            \"ApoplecticWarg\",\n",
    "            \"Safe\",\n",
    "            \"Sorry\",\n",
    "        ]\n",
    "\n",
    "        # Possible actions\n",
    "        self.actions = [\"pet\", \"strike\"]\n",
    "\n",
    "        # Transition probabilities and rewards\n",
    "        # transitions[state][action] = [(next_state, probability, reward), ...]\n",
    "        self.transitions = {\n",
    "            \"SleepingWarg\": {\n",
    "                \"pet\": [(\"AngryWarg\", 0.95, -1), (\"Safe\", 0.05, 10)],\n",
    "                \"strike\": [(\"AngryWarg\", 1.0, -1)],\n",
    "            },\n",
    "            \"AngryWarg\": {\n",
    "                \"pet\": [(\"Sorry\", 1.0, -10)],\n",
    "                \"strike\": [(\"FuriousWarg\", 1.0, -1)],\n",
    "            },\n",
    "            \"FuriousWarg\": {\n",
    "                \"pet\": [(\"Sorry\", 1.0, -10)],\n",
    "                \"strike\": [(\"ApoplecticWarg\", 1.0, -1)],\n",
    "            },\n",
    "            \"ApoplecticWarg\": {\n",
    "                \"pet\": [(\"Sorry\", 1.0, -10)],\n",
    "                \"strike\": [(\"Safe\", 0.2, 10), (\"Sorry\", 0.8, -10)],\n",
    "            },\n",
    "            \"Safe\": {},\n",
    "            \"Sorry\": {},\n",
    "        }\n",
    "\n",
    "        # Discount factor\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    # Returns all of the states\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    # Returns all of the actions for a given state\n",
    "    def get_actions(self, state):\n",
    "        # If the state is terminal, no actions are possible\n",
    "        if state in [\"Safe\", \"Sorry\"]:\n",
    "            return []\n",
    "        return self.actions\n",
    "\n",
    "    # Returns the list of (next_state, probability, reward) for a given state and action\n",
    "    def get_transitions(self, state, action):\n",
    "        if state in self.transitions and action in self.transitions[state]:\n",
    "            return self.transitions[state][action]\n",
    "        return []\n",
    "\n",
    "    # Checks if a state is terminal\n",
    "    def is_terminal(self, state):\n",
    "        return state in [\"Safe\", \"Sorry\"]\n",
    "\n",
    "# Print out the MDP\n",
    "mdp = WargMDP()\n",
    "print(\"States:\", mdp.get_states())\n",
    "print(\"Actions in 'SleepingWarg':\", mdp.get_actions(\"SleepingWarg\"))\n",
    "print(\"Transitions for petting 'SleepingWarg':\", mdp.get_transitions(\"SleepingWarg\", \"pet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2: Warg as an MDP\n",
    "Implement the WargPettingGame as an MDP using the implementation from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: (SleepingWarg, Reward: 0, Done: False)\n",
      "Available actions: ['pet', 'strike']\n",
      "Taking action: pet\n",
      "\n",
      "Next state: (AngryWarg, Reward: -1, Done: False)\n",
      "Available actions: ['pet', 'strike']\n",
      "Taking action: pet\n",
      "\n",
      "Next state: (Sorry, Reward: -11, Done: True)\n"
     ]
    }
   ],
   "source": [
    "# ChatGPT-4o\n",
    "# Provided the MDP class code and asked it to generate the code for the WargPettingGame class.\n",
    "# Had to tweak the step and probability_transition methods.\n",
    "\n",
    "class WargPettingGame:\n",
    "    def __init__(self):\n",
    "        self.mdp = WargMDP()\n",
    "        self.current_state = \"SleepingWarg\"\n",
    "\n",
    "    # Resets the game to the initial state\n",
    "    def reset(self):\n",
    "        self.current_state = \"SleepingWarg\"\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Check if the current state is terminal\n",
    "        if self.mdp.is_terminal(self.current_state):\n",
    "            raise ValueError(\"Cannot take an action in a terminal state.\")\n",
    "\n",
    "        # Check if the action is valid in the current state\n",
    "        transitions = self.mdp.get_transitions(self.current_state, action)\n",
    "        if not transitions:\n",
    "            raise ValueError(f\"Invalid action '{action}' in state '{self.current_state}'.\")\n",
    "\n",
    "        # Get the next state and reward based on the transition probabilities\n",
    "        next_state, reward = self.probability_transition(transitions)\n",
    "        self.current_state = next_state\n",
    "\n",
    "        # Mark the episode as done if the next state is terminal\n",
    "        done = self.mdp.is_terminal(next_state)\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    # Returns the available actions in the current state\n",
    "    def get_available_actions(self):\n",
    "        return self.mdp.get_actions(self.current_state)\n",
    "\n",
    "    # Decides the next transition based on the next transition probabilities\n",
    "    def probability_transition(self, transitions):\n",
    "        import random\n",
    "\n",
    "        rand_val = random.random()\n",
    "        cumulative_probability = 0.0\n",
    "\n",
    "        # Simulate probabilistic state transitions\n",
    "        for next_state, probability, reward in transitions:\n",
    "            cumulative_probability += probability\n",
    "            if rand_val < cumulative_probability:\n",
    "                return next_state, reward\n",
    "\n",
    "        # Fallback in case of numerical issues\n",
    "        return transitions[-1][0], transitions[-1][2]\n",
    "\n",
    "# Print out the game\n",
    "game = WargPettingGame()\n",
    "state = game.reset()\n",
    "total = 0\n",
    "done = False\n",
    "\n",
    "print(f\"Initial state: ({state}, Reward: {total}, Done: {done})\")\n",
    "\n",
    "while not done:\n",
    "    actions = game.get_available_actions()\n",
    "    print(f\"Available actions: {actions}\")\n",
    "\n",
    "    # Choose the first available action\n",
    "    action = actions[0]\n",
    "    print(f\"Taking action: {action}\")\n",
    "\n",
    "    state, reward, done = game.step(action)\n",
    "    total += reward\n",
    "    print(f\"\\nNext state: ({state}, Reward: {total}, Done: {done})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3: Value iteration\n",
    "\n",
    "Implement the value iteration as a separate function that uses this MDP implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT-4o\n",
    "# Provided the MDP class code and asked it to generate the code for the value_iteration function.\n",
    "# Had to tweak the computation for the action value and finding the best action and policy.\n",
    "\n",
    "def value_iteration(mdp, epsilon):\n",
    "    # Initialize value function for all states to 0\n",
    "    value_function = {state: 0 for state in mdp.get_states()}\n",
    "    policy = {}\n",
    "\n",
    "    # Repeat until convergence of the value function\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_value_function = value_function.copy()\n",
    "\n",
    "        # Iterate over all states\n",
    "        for state in mdp.get_states():\n",
    "            if mdp.is_terminal(state):\n",
    "                continue\n",
    "\n",
    "            best_value = float(\"-inf\")\n",
    "            best_action = None\n",
    "\n",
    "            # Find the best action for a state\n",
    "            for action in mdp.get_actions(state):\n",
    "                action_value = 0\n",
    "\n",
    "                # Calculate the expected value of an action\n",
    "                for next_state, probability, reward in mdp.get_transitions(state, action):\n",
    "                    action_value += probability * (reward + mdp.gamma * value_function[next_state])\n",
    "\n",
    "                if action_value > best_value:\n",
    "                    best_value = action_value\n",
    "                    best_action = action\n",
    "\n",
    "            # Update the value function and policy\n",
    "            new_value_function[state] = best_value\n",
    "            policy[state] = best_action\n",
    "\n",
    "            # Update the maximum change for convergence check\n",
    "            delta = max(delta, abs(new_value_function[state] - value_function[state]))\n",
    "\n",
    "        value_function = new_value_function\n",
    "\n",
    "        # Check for convergence\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "\n",
    "    return policy, value_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4: Using value iteration\n",
    "Find the V* values of the WargPettingGame using the implementation above. Print out the V* values for each state in the form \n",
    "V(state) == number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function \n",
      "\n",
      "V(SleepingWarg) = -6.23\n",
      "V(AngryWarg) = -6.76\n",
      "V(FuriousWarg) = -6.40\n",
      "V(ApoplecticWarg) = -6.00\n",
      "V(Safe) = 0.00\n",
      "V(Sorry) = 0.00\n"
     ]
    }
   ],
   "source": [
    "# Print the optimal value function\n",
    "mdp = WargMDP()\n",
    "epsilon = 1e-6\n",
    "optimal_policy, optimal_value_function = value_iteration(mdp, epsilon)\n",
    "\n",
    "print(\"Optimal Value Function\", \"\\n\")\n",
    "for state, value in optimal_value_function.items():\n",
    "    print(f\"V({state}) = {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P5:  Policy extraction\n",
    "\n",
    "Find the policy $\\pi(s)$ from the V values obtained in the previous step. Remember that you need to do one step of expectimax.\n",
    "Print out the policy for each state, in a readable way. Eg. \n",
    "    pi(ApoplecticWarg) = Pet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy \n",
      "\n",
      "π(SleepingWarg) = pet\n",
      "π(AngryWarg) = strike\n",
      "π(FuriousWarg) = strike\n",
      "π(ApoplecticWarg) = strike\n"
     ]
    }
   ],
   "source": [
    "# Print the optimal policy\n",
    "mdp = WargMDP()\n",
    "epsilon = 1e-6\n",
    "optimal_policy, optimal_value_function = value_iteration(mdp, epsilon)\n",
    "\n",
    "print(\"Optimal Policy\", \"\\n\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"π({state}) = {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P6: Policy iteration\n",
    "Implement policy iteration with the MDP as defined above as a separate function.\n",
    "Apply it to the MDP defining the pet the warg game. \n",
    "Print out the resulting policy for each state, in a readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy \n",
      "\n",
      "π(SleepingWarg) = Pet\n",
      "π(AngryWarg) = Strike\n",
      "π(FuriousWarg) = Strike\n",
      "π(ApoplecticWarg) = Strike\n"
     ]
    }
   ],
   "source": [
    "# ChatGPT-4o\n",
    "# Provided the MDP class code and asked it to generate the code for the policy_iteration function.\n",
    "# Had to modifiy the policy update based on the value section of the code.\n",
    "\n",
    "def policy_iteration(mdp, epsilon):\n",
    "    # Initialize policy arbitrarily with the pet action\n",
    "    policy = {state: \"pet\" for state in mdp.get_states() if not mdp.is_terminal(state)}\n",
    "    value_function = {state: 0 for state in mdp.get_states()}\n",
    "\n",
    "    while True:\n",
    "        # Update the value function using the current policy\n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_value_function = value_function.copy()\n",
    "\n",
    "            for state in mdp.get_states():\n",
    "                if mdp.is_terminal(state):\n",
    "                    continue\n",
    "\n",
    "                # Get the action from the current policy\n",
    "                action = policy[state]\n",
    "                action_value = 0\n",
    "\n",
    "                # Calculate the expected value of an action from the current policy\n",
    "                for next_state, probability, reward in mdp.get_transitions(state, action):\n",
    "                    action_value += probability * (reward + mdp.gamma * value_function[next_state])\n",
    "\n",
    "                new_value_function[state] = action_value\n",
    "                delta = max(delta, abs(new_value_function[state] - value_function[state]))\n",
    "\n",
    "            value_function = new_value_function\n",
    "\n",
    "            if delta < epsilon:\n",
    "                break\n",
    "\n",
    "        # Update the policy based on the updated value function if necessary\n",
    "        policy_stable = True\n",
    "        for state in mdp.get_states():\n",
    "            if mdp.is_terminal(state):\n",
    "                continue\n",
    "\n",
    "            best_action = None\n",
    "            best_value = float(\"-inf\")\n",
    "\n",
    "            # Find the best action for a state\n",
    "            for action in mdp.get_actions(state):\n",
    "                action_value = 0\n",
    "\n",
    "                # Calculate the expected value of an action\n",
    "                for next_state, probability, reward in mdp.get_transitions(state, action):\n",
    "                    action_value += probability * (reward + mdp.gamma * value_function[next_state])\n",
    "\n",
    "                if action_value > best_value:\n",
    "                    best_value = action_value\n",
    "                    best_action = action\n",
    "\n",
    "            # Update the policy if the best action has changed\n",
    "            if best_action != policy[state]:\n",
    "                policy_stable = False\n",
    "                policy[state] = best_action\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "# Print the optimal policy\n",
    "mdp = WargMDP()\n",
    "epsilon = 1e-6\n",
    "optimal_policy = policy_iteration(mdp, epsilon)\n",
    "\n",
    "print(\"Optimal Policy\", \"\\n\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"π({state}) = {action.capitalize() if action else 'Terminal'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P7: Trajectory sampling\n",
    "Implement a function that generates trajectories in the form of (s,a,r,s') tuples from the MDP for a specific policy. The trajectory ends when it reaches a terminal state. \n",
    "\n",
    "Generate 100 trajectories for a __random__ policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Trajectories \n",
      "\n",
      "Trajectory 1:\n",
      "('SleepingWarg', 'pet', 10, 'Safe')\n",
      "\n",
      "Trajectory 2:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 3:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 4:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 5:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 6:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 7:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 8:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 9:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 10:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 11:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 12:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 13:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 14:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'strike', -10, 'Sorry')\n",
      "\n",
      "Trajectory 15:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 16:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 17:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 18:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 19:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 20:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 21:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 22:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 23:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 24:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 25:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 26:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 27:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 28:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 29:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 30:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 31:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 32:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 33:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 34:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 35:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 36:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 37:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 38:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'strike', -10, 'Sorry')\n",
      "\n",
      "Trajectory 39:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 40:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'strike', -10, 'Sorry')\n",
      "\n",
      "Trajectory 41:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 42:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 43:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 44:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'strike', -10, 'Sorry')\n",
      "\n",
      "Trajectory 45:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 46:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 47:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 48:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 49:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 50:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'strike', -10, 'Sorry')\n",
      "\n",
      "Trajectory 51:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 52:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 53:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 54:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 55:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 56:\n",
      "('SleepingWarg', 'pet', 10, 'Safe')\n",
      "\n",
      "Trajectory 57:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'strike', -10, 'Sorry')\n",
      "\n",
      "Trajectory 58:\n",
      "('SleepingWarg', 'pet', 10, 'Safe')\n",
      "\n",
      "Trajectory 59:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 60:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 61:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 62:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 63:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 64:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 65:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 66:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 67:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 68:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 69:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 70:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 71:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 72:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 73:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 74:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 75:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'strike', -10, 'Sorry')\n",
      "\n",
      "Trajectory 76:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 77:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 78:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 79:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 80:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 81:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 82:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 83:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 84:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 85:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 86:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 87:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'strike', -10, 'Sorry')\n",
      "\n",
      "Trajectory 88:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 89:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 90:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 91:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 92:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 93:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 94:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'pet', -10, 'Sorry')\n",
      "\n",
      "Trajectory 95:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'strike', -10, 'Sorry')\n",
      "\n",
      "Trajectory 96:\n",
      "('SleepingWarg', 'pet', 10, 'Safe')\n",
      "\n",
      "Trajectory 97:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'strike', -10, 'Sorry')\n",
      "\n",
      "Trajectory 98:\n",
      "('SleepingWarg', 'strike', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'strike', -10, 'Sorry')\n",
      "\n",
      "Trajectory 99:\n",
      "('SleepingWarg', 'pet', -1, 'AngryWarg')\n",
      "('AngryWarg', 'strike', -1, 'FuriousWarg')\n",
      "('FuriousWarg', 'strike', -1, 'ApoplecticWarg')\n",
      "('ApoplecticWarg', 'strike', -10, 'Sorry')\n",
      "\n",
      "Trajectory 100:\n",
      "('SleepingWarg', 'pet', 10, 'Safe')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Generate a number of trajectories in the form of (s, a, r, s') using a random policy\n",
    "def generate_random_trajectories(mdp, num_trajectories):\n",
    "    trajectories = []\n",
    "\n",
    "    for _ in range(num_trajectories):\n",
    "        # Start from the initial state\n",
    "        trajectory = []\n",
    "        state = \"SleepingWarg\"\n",
    "\n",
    "        while not mdp.is_terminal(state):\n",
    "            # Choose an action randomly\n",
    "            actions = mdp.get_actions(state)\n",
    "            action = random.choice(actions)\n",
    "\n",
    "            # Get the transition probabilities and rewards for the chosen action\n",
    "            transitions = mdp.get_transitions(state, action)\n",
    "\n",
    "            # Determine the next state and reward based on probabilities\n",
    "            rand_val = random.random()\n",
    "            cumulative_probability = 0.0\n",
    "\n",
    "            for next_state, probability, reward in transitions:\n",
    "                cumulative_probability += probability\n",
    "                if rand_val < cumulative_probability:\n",
    "                    trajectory.append((state, action, reward, next_state))\n",
    "                    state = next_state\n",
    "                    break\n",
    "\n",
    "        # Add the trajectory to the list\n",
    "        trajectories.append(trajectory)\n",
    "\n",
    "    return trajectories\n",
    "\n",
    "# Print the sample trajectories\n",
    "mdp = WargMDP()\n",
    "num_trajectories = 100\n",
    "random_trajectories = generate_random_trajectories(mdp, num_trajectories)\n",
    "\n",
    "print(\"Sample Trajectories\", \"\\n\")\n",
    "for i, trajectory in enumerate(random_trajectories, start=1):\n",
    "    print(f\"Trajectory {i}:\")\n",
    "    for step in trajectory:\n",
    "        print(f\"{step}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P8: Implement Q-learning \n",
    "\n",
    "Create an implementation of Q-learning which takes the trajectory database and updates a Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT-4o\n",
    "# Provided the generate_random_trajectories function and asked to generate the code for the q_learning function.\n",
    "# Had to tweak the constants and Q-value update formula based on the Q-learning update rule.\n",
    "\n",
    "def q_learning(mdp, trajectories, alpha, gamma):\n",
    "    # Initialize Q-table\n",
    "    # Q[(state, action)] = Q-value\n",
    "    Q = {}\n",
    "\n",
    "    # Initialize Q-values for each state-action pair to 0\n",
    "    for state in mdp.get_states():\n",
    "        for action in mdp.get_actions(state):\n",
    "            Q[(state, action)] = 0.0  # Initial Q-value is 0\n",
    "\n",
    "    # Iterate through each trajectory\n",
    "    for trajectory in trajectories:\n",
    "        for step in trajectory:\n",
    "            state, action, reward, next_state = step\n",
    "\n",
    "            # If the next state is terminal, the Q-update will be based solely on the reward\n",
    "            if mdp.is_terminal(next_state):\n",
    "                future_q = 0\n",
    "            else:\n",
    "                # Get the maximum Q-value for the next state over all possible actions\n",
    "                future_q = max(Q.get((next_state, a), 0.0) for a in mdp.get_actions(next_state))\n",
    "\n",
    "            # Update the Q-value using the Q-learning update rule\n",
    "            Q[(state, action)] = Q[(state, action)] + alpha * (reward + gamma * future_q - Q[(state, action)])\n",
    "\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P9: Run Q-learning \n",
    "\n",
    "Run your implementation of Q-learning on the warg petting game. Print out the Q values in the form \n",
    "\n",
    "Q(state, action) = number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values \n",
      "\n",
      "Q(SleepingWarg, pet) = -3.24\n",
      "Q(SleepingWarg, strike) = -3.72\n",
      "Q(AngryWarg, pet) = -9.91\n",
      "Q(AngryWarg, strike) = -3.72\n",
      "Q(FuriousWarg, pet) = -9.20\n",
      "Q(FuriousWarg, strike) = -3.34\n",
      "Q(ApoplecticWarg, pet) = -8.15\n",
      "Q(ApoplecticWarg, strike) = -0.87\n"
     ]
    }
   ],
   "source": [
    " # Initialize the Warg Petting Game\n",
    "game = WargPettingGame()\n",
    "mdp = game.mdp\n",
    "\n",
    "# Generate trajectories using a random policy\n",
    "random_trajectories = generate_random_trajectories(mdp, num_trajectories)\n",
    "\n",
    "    # Run Q-learning with specified parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = mdp.gamma  # Discount factor\n",
    "Q_table = q_learning(mdp, random_trajectories, alpha, gamma)\n",
    "\n",
    "    # Print the Q-values in the specified format\n",
    "print(\"Q-values\", \"\\n\")\n",
    "for state in mdp.get_states():\n",
    "    for action in mdp.get_actions(state):\n",
    "        print(f\"Q({state}, {action}) = {Q_table[(state, action)]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P10: Policy implied by Q-values\n",
    "\n",
    "Write a function that extracts a policy form q-values. \n",
    "Apply it to the Q-table obtained at P9. Print out the resulting policy in a readable way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy \n",
      "\n",
      "π(SleepingWarg) = pet\n",
      "π(AngryWarg) = strike\n",
      "π(FuriousWarg) = strike\n",
      "π(ApoplecticWarg) = strike\n"
     ]
    }
   ],
   "source": [
    "def extract_policy(Q, mdp):\n",
    "    # Initialize an empty policy\n",
    "    # policy[state] = action\n",
    "    policy = {}\n",
    "\n",
    "    # For each state, find the action with the highest Q-value\n",
    "    for state in mdp.get_states():\n",
    "        if not mdp.is_terminal(state):\n",
    "            # Get the action with the highest Q-value for the given state\n",
    "            best_action = max(mdp.get_actions(state), key=lambda a: Q[(state, a)])\n",
    "            policy[state] = best_action\n",
    "\n",
    "    return policy\n",
    "\n",
    "# Apply the policy extraction function to the Q-table from Q-learning\n",
    "policy = extract_policy(Q_table, mdp)\n",
    "\n",
    "# Print the optimal policy\n",
    "print(\"Optimal Policy\", \"\\n\")\n",
    "for state, action in policy.items():\n",
    "    print(f\"π({state}) = {action}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
