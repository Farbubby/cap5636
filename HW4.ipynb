{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Farhan Mahbub\n",
    "# CAP5636 - Advanced AI\n",
    "# November 17, 2024\n",
    "# Homework 4: Petting a warg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Petting a warg\n",
    "\n",
    "Wargs do not make good pets. They are vicious creatures, populating Middle Earth, the world described by novels of John Ronald Reuel Tolkien. They tend to show up in the worst moment possible. They eat humans, hobbits, elves and wizards (when they can get them).\n",
    "\n",
    "![A warg, getting ready for breakfast w:300px](figures/Gundabad_Wargs.jpg)\n",
    "\n",
    "Your relationship with a warg can be in the following states:\n",
    "```\n",
    "SleepingWarg\n",
    "AngryWarg\n",
    "FuriousWarg\n",
    "ApoplecticWarg\n",
    "Safe\n",
    "Sorry \n",
    "```\n",
    "\n",
    "![tes](figures/WargStates.jpg)\n",
    "\n",
    "Your actions are limited to petting a warg or striking it with your sword. The transitions are described in the following picture. The safe and sorry states are terminal, where no further actions can be taken. Landing into them has the reward +10 and -10 respectively. All other actions have a reward of -1. \n",
    "\n",
    "The discount factor is $\\gamma=0.9$\n",
    "\n",
    "![](figures/PetAWarg.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to solve this homework\n",
    "The following problems you can solve either with the help of an LLM or by hand. \n",
    "\n",
    "* If you are solving by hand, make sure that you add sufficient comments to make sure that the code is understandable. \n",
    "* If you are solving using an LLM, add in form of comments\n",
    "    * the LLM used (at the first use instance)\n",
    "    * the prompt used to elicit the code\n",
    "    * modifications that had to be done to the code \n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "# --- LLM used: ChatGPT 4.5\n",
    "# --- LLM prompt\n",
    "# Write a python class to encapsulate the least common multiple algorithm\n",
    "# --- End of LLM prompt\n",
    "```\n",
    "\n",
    "The programming language should be Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1: MDP implementation \n",
    "\n",
    "Write a class to implement an MDP. Do not include value or policy iteration in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: ['SleepingWarg', 'AngryWarg', 'FuriousWarg', 'ApoplecticWarg', 'Safe', 'Sorry']\n",
      "Actions in 'SleepingWarg': ['pet', 'strike']\n",
      "Transitions for petting 'SleepingWarg': [('AngryWarg', 0.95, -1), ('Safe', 0.05, 10)]\n"
     ]
    }
   ],
   "source": [
    "class WargMDP:\n",
    "    def __init__(self):\n",
    "        # Define the states\n",
    "        self.states = [\n",
    "            \"SleepingWarg\",\n",
    "            \"AngryWarg\",\n",
    "            \"FuriousWarg\",\n",
    "            \"ApoplecticWarg\",\n",
    "            \"Safe\",\n",
    "            \"Sorry\",\n",
    "        ]\n",
    "\n",
    "        # Possible actions                                   actions\n",
    "        self.actions = [\"pet\", \"strike\"]\n",
    "\n",
    "        # Transition probabilities and rewards\n",
    "        # transitions[state][action] = [(next_state, probability, reward), ...]\n",
    "        self.transitions = {\n",
    "            \"SleepingWarg\": {\n",
    "                \"pet\": [(\"AngryWarg\", 0.95, -1), (\"Safe\", 0.05, 10)],\n",
    "                \"strike\": [(\"AngryWarg\", 1.0, -1)],\n",
    "            },\n",
    "            \"AngryWarg\": {\n",
    "                \"pet\": [(\"Sorry\", 1.0, -10)],\n",
    "                \"strike\": [(\"FuriousWarg\", 1.0, -1)],\n",
    "            },\n",
    "            \"FuriousWarg\": {\n",
    "                \"pet\": [(\"Sorry\", 1.0, -10)],\n",
    "                \"strike\": [(\"ApoplecticWarg\", 1.0, -1)],\n",
    "            },\n",
    "            \"ApoplecticWarg\": {\n",
    "                \"pet\": [(\"Sorry\", 1.0, -10)],\n",
    "                \"strike\": [(\"Safe\", 0.2, 10), (\"Sorry\", 0.8, -10)],\n",
    "            },\n",
    "            \"Safe\": {},\n",
    "            \"Sorry\": {},\n",
    "        }\n",
    "\n",
    "        # Discount factor\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    # Returns all of the states\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    # Returns all of the actions for a given state\n",
    "    def get_actions(self, state):\n",
    "        # If the state is terminal, no actions are possible\n",
    "        if state in [\"Safe\", \"Sorry\"]:\n",
    "            return []\n",
    "        return self.actions\n",
    "\n",
    "    # Returns the list of (next_state, probability, reward) for a given state and action\n",
    "    def get_transitions(self, state, action):\n",
    "        if state in self.transitions and action in self.transitions[state]:\n",
    "            return self.transitions[state][action]\n",
    "        return []\n",
    "\n",
    "    # Checks if a state is terminal\n",
    "    def is_terminal(self, state):\n",
    "        return state in [\"Safe\", \"Sorry\"]\n",
    "\n",
    "# Test\n",
    "mdp = WargMDP()\n",
    "print(\"States:\", mdp.get_states())\n",
    "print(\"Actions in 'SleepingWarg':\", mdp.get_actions(\"SleepingWarg\"))\n",
    "print(\"Transitions for petting 'SleepingWarg':\", mdp.get_transitions(\"SleepingWarg\", \"pet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2: Warg as an MDP\n",
    "Implement the WargPettingGame as an MDP using the implementation from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: SleepingWarg, Reward: 0, Done: False\n",
      "Available actions: ['pet', 'strike']\n",
      "Taking action: pet\n",
      "\n",
      "Next state: AngryWarg, Reward: -1, Done: False\n",
      "Available actions: ['pet', 'strike']\n",
      "Taking action: pet\n",
      "\n",
      "Next state: Sorry, Reward: -11, Done: True\n"
     ]
    }
   ],
   "source": [
    "class WargPettingGame:\n",
    "    def __init__(self):\n",
    "        self.mdp = WargMDP()\n",
    "        self.current_state = \"SleepingWarg\"\n",
    "\n",
    "    # Resets the game to the initial state\n",
    "    def reset(self):\n",
    "        self.current_state = \"SleepingWarg\"\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Check if the current state is terminal\n",
    "        if self.mdp.is_terminal(self.current_state):\n",
    "            raise ValueError(\"Cannot take an action in a terminal state.\")\n",
    "\n",
    "        # Check if the action is valid in the current state\n",
    "        transitions = self.mdp.get_transitions(self.current_state, action)\n",
    "        if not transitions:\n",
    "            raise ValueError(f\"Invalid action '{action}' in state '{self.current_state}'.\")\n",
    "\n",
    "        # Get the next state and reward based on the transition probabilities\n",
    "        next_state, reward = self.probability_transition(transitions)\n",
    "        self.current_state = next_state\n",
    "\n",
    "        # Mark the episode as done if the next state is terminal\n",
    "        done = self.mdp.is_terminal(next_state)\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    # Returns the available actions in the current state\n",
    "    def get_available_actions(self):\n",
    "        return self.mdp.get_actions(self.current_state)\n",
    "\n",
    "    # Decides the next transition based on the next transition probabilities\n",
    "    def probability_transition(self, transitions):\n",
    "        import random\n",
    "\n",
    "        rand_val = random.random()\n",
    "        cumulative_probability = 0.0\n",
    "\n",
    "        # Simulate probabilistic state transitions\n",
    "        for next_state, probability, reward in transitions:\n",
    "            cumulative_probability += probability\n",
    "            if rand_val < cumulative_probability:\n",
    "                return next_state, reward\n",
    "\n",
    "        # Fallback in case of numerical issues\n",
    "        return transitions[-1][0], transitions[-1][2]\n",
    "\n",
    "# Test\n",
    "game = WargPettingGame()\n",
    "state = game.reset()\n",
    "total = 0\n",
    "done = False\n",
    "\n",
    "print(f\"Initial state: {state}, Reward: {total}, Done: {done}\")\n",
    "\n",
    "while not done:\n",
    "    actions = game.get_available_actions()\n",
    "    print(f\"Available actions: {actions}\")\n",
    "\n",
    "    # Choose the first available action\n",
    "    action = actions[0]\n",
    "    print(f\"Taking action: {action}\")\n",
    "\n",
    "    state, reward, done = game.step(action)\n",
    "    total += reward\n",
    "    print(f\"\\nNext state: {state}, Reward: {total}, Done: {done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3: Value iteration\n",
    "\n",
    "Implement the value iteration as a separate function that uses this MDP implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4: Using value iteration\n",
    "Find the V* values of the WargPettingGame using the implementation above. Print out the V* values for each state in the form \n",
    "V(state) == number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P5:  Policy extraction\n",
    "\n",
    "Find the policy $\\pi(s)$ from the V values obtained in the previous step. Remember that you need to do one step of expectimax.\n",
    "Print out the policy for each state, in a readable way. Eg. \n",
    "    pi(ApoplecticWarg) = Pet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P6: Policy iteration\n",
    "Implement policy iteration with the MDP as defined above as a separate function.\n",
    "Apply it to the MDP defining the pet the warg game. \n",
    "Print out the resulting policy for each state, in a readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P7: Trajectory sampling\n",
    "Implement a function that generates trajectories in the form of (s,a,r,s') tuples from the MDP for a specific policy. The trajectory ends when it reaches a terminal state. \n",
    "\n",
    "Generate 100 trajectories for a __random__ policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P8: Implement Q-learning \n",
    "\n",
    "Create an implementation of Q-learning which takes the trajectory database and updates a Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P9: Run Q-learning \n",
    "\n",
    "Run your implementation of Q-learning on the warg petting game. Print out the Q values in the form \n",
    "\n",
    "Q(state, action) = number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P10: Policy implied by Q-values\n",
    "\n",
    "Write a function that extracts a policy form q-values. \n",
    "Apply it to the Q-table obtained at P9. Print out the resulting policy in a readable way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
